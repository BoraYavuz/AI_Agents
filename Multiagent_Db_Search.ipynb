{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjJYTtR/tPHkSWlqfGOpmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoraYavuz/AI_Agents/blob/main/Multiagent_Db_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain_community\n",
        "!pip install arxiv\n",
        "!pip install plotly\n",
        "!pip install scholarly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "uK68CA3g4Ixo",
        "outputId": "45031977-e416-4361-a921-52538bc7352c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.42.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.65)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (2.11.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.11/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from scholarly) (4.13.4)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.3.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from scholarly)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.3.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from scholarly) (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from scholarly) (1.1.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from scholarly) (2.32.3)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scholarly) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->scholarly) (2.9.0.post0)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->scholarly) (2.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from bibtexparser->scholarly) (3.2.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->scholarly) (1.17.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from free-proxy->scholarly) (5.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->scholarly) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (2.4.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.30.0 (from selenium->scholarly)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting typing-extensions (from scholarly)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.19.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium->scholarly) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium->scholarly) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium->scholarly) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.2)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.3-py3-none-any.whl size=43549 sha256=d1501e9a2df8d2a4dd94b4bb4db072c1b14e4c0034a083cedd5f1ac33d9cd672\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/fb/76/306387739cf9d53b1c39b0c8aadbbb17dc05f256756d8fd915\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.3-py3-none-any.whl size=6097 sha256=a94236f0415de3854ecc5ad6af991d07c11c92cd12b3e6839c6072cf29cc15d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/f8/90/1e74c4166b7fbb213260a35e83fac3119f5e390c8bddda8a37\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: wsproto, typing-extensions, types-python-dateutil, outcome, fake-useragent, deprecated, bibtexparser, trio, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, sphinx-rtd-theme, selenium, scholarly\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 bibtexparser-1.4.3 deprecated-1.2.18 fake-useragent-2.2.0 free-proxy-1.1.3 outcome-1.3.0.post0 scholarly-1.7.11 selenium-4.33.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 trio-0.30.0 trio-websocket-0.12.2 types-python-dateutil-2.9.0.20250516 typing-extensions-4.13.2 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "2a159aa1d0d14263bf17b9b20c35bc6b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import Tool\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_community.tools import WikipediaQueryRun, DuckDuckGoSearchRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper, DuckDuckGoSearchAPIWrapper\n",
        "import arxiv\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from scholarly import scholarly\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Any"
      ],
      "metadata": {
        "id": "OgbE3g8l41V0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Multi-Agent Research System\",\n",
        "    page_icon=\"ðŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Sidebar configuration\n",
        "st.sidebar.title(\"ðŸ”§ Configuration\")\n",
        "openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "\n",
        "# Agent selection\n",
        "st.sidebar.markdown(\"### ðŸ¤– Agent Selection\")\n",
        "agent_pipeline = st.sidebar.multiselect(\n",
        "    \"Select agents to include in pipeline:\",\n",
        "    [\"Literature Agent\", \"Analysis Agent\", \"Writing Agent\", \"Fact-Check Agent\", \"Visualization Agent\"],\n",
        "    default=[\"Literature Agent\", \"Analysis Agent\", \"Writing Agent\"]\n",
        ")\n",
        "\n",
        "# Database selection (keeping your original code)\n",
        "st.sidebar.markdown(\"### ðŸ“š Research Database\")\n",
        "database_options = {\n",
        "    \"ArXiv\": \"Academic preprints (Physics, Math, CS, etc.)\",\n",
        "    \"Google Scholar\": \"Broad academic search across disciplines\",\n",
        "    \"PubMed\": \"Medical and life sciences literature\",\n",
        "    \"Semantic Scholar\": \"AI-powered academic search\",\n",
        "    \"CrossRef\": \"Academic publications with DOIs\"\n",
        "}\n",
        "\n",
        "selected_db = st.sidebar.selectbox(\n",
        "    \"Choose Research Database:\",\n",
        "    options=list(database_options.keys()),\n",
        "    help=\"Different databases specialize in different fields\"\n",
        ")\n",
        "\n",
        "max_papers = st.sidebar.slider(\"Max papers to analyze\", 5, 50, 10)\n",
        "\n",
        "# [Keep all your original search functions here - they're perfect]\n",
        "def search_arxiv_papers(query, max_results=10):\n",
        "    \"\"\"Search arXiv for papers\"\"\"\n",
        "    try:\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=max_results,\n",
        "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "        )\n",
        "        papers = []\n",
        "        for paper in search.results():\n",
        "            papers.append({\n",
        "                'title': paper.title,\n",
        "                'authors': [author.name for author in paper.authors],\n",
        "                'published': paper.published.strftime('%Y-%m-%d'),\n",
        "                'summary': paper.summary,\n",
        "                'url': paper.entry_id,\n",
        "                'categories': paper.categories,\n",
        "                'source': 'ArXiv'\n",
        "            })\n",
        "        return papers\n",
        "    except Exception as e:\n",
        "        st.error(f\"ArXiv search error: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tiCamzfw56m7",
        "outputId": "412ad86d-3b14-4763-eba2-e9a27b7c91dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-17 12:41:31.022 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.024 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.305 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-06-17 12:41:31.307 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.308 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.309 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.310 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.312 Session state does not function when running a script without `streamlit run`\n",
            "2025-06-17 12:41:31.314 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.315 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.316 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.318 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.322 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.324 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.326 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.329 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.332 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.333 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 12:41:31.337 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Keep all your original search functions here - they're perfect]\n",
        "def search_arxiv_papers(query, max_results=10):\n",
        "    \"\"\"Search arXiv for papers\"\"\"\n",
        "    try:\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=max_results,\n",
        "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "        )\n",
        "        papers = []\n",
        "        for paper in search.results():\n",
        "            papers.append({\n",
        "                'title': paper.title,\n",
        "                'authors': [author.name for author in paper.authors],\n",
        "                'published': paper.published.strftime('%Y-%m-%d'),\n",
        "                'summary': paper.summary,\n",
        "                'url': paper.entry_id,\n",
        "                'categories': paper.categories,\n",
        "                'source': 'ArXiv'\n",
        "            })\n",
        "        return papers\n",
        "    except Exception as e:\n",
        "        st.error(f\"ArXiv search error: {e}\")\n",
        "        return []\n",
        "\n",
        "# [Include all your other search functions - keeping them for brevity]\n",
        "def search_papers_by_database(database, query, max_results=10):\n",
        "    \"\"\"Route search to appropriate database function\"\"\"\n",
        "    if database == \"ArXiv\":\n",
        "        return search_arxiv_papers(query, max_results)\n",
        "    # Add other database calls here\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# NEW: Enhanced Agent Classes\n",
        "class MultiAgentOrchestrator:\n",
        "    def __init__(self, api_key: str, selected_database: str):\n",
        "        self.api_key = api_key\n",
        "        self.selected_database = selected_database\n",
        "        self.llm = ChatOpenAI(\n",
        "            temperature=0.1,\n",
        "            openai_api_key=api_key,\n",
        "            model=\"gpt-4o-mini\"\n",
        "        )\n",
        "        self.shared_memory = {}\n",
        "\n",
        "    def create_literature_agent(self):\n",
        "        \"\"\"Your original literature agent with slight modifications\"\"\"\n",
        "        def paper_search_tool(query: str) -> str:\n",
        "            papers = search_papers_by_database(self.selected_database, query, max_papers)\n",
        "            if not papers:\n",
        "                return f\"No papers found for '{query}' in {self.selected_database}.\"\n",
        "\n",
        "            # Store papers in shared memory for other agents\n",
        "            self.shared_memory['papers'] = papers\n",
        "            self.shared_memory['search_query'] = query\n",
        "\n",
        "            result = f\"Found {len(papers)} papers on '{query}' from {self.selected_database}:\\n\\n\"\n",
        "            for i, paper in enumerate(papers, 1):\n",
        "                result += f\"{i}. **{paper['title']}**\\n\"\n",
        "                result += f\"   Authors: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}\\n\"\n",
        "                result += f\"   Published: {paper['published']}\\n\"\n",
        "                result += f\"   Summary: {paper['summary'][:200]}...\\n\\n\"\n",
        "\n",
        "            return result\n",
        "\n",
        "        tools = [Tool(\n",
        "            name=\"paper_search\",\n",
        "            func=paper_search_tool,\n",
        "            description=f\"Search for academic papers using {self.selected_database}\"\n",
        "        )]\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", f\"\"\"You are a research literature agent specialized in {self.selected_database}.\n",
        "            Your role is to search for and summarize academic papers. Be thorough and accurate.\"\"\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"assistant\", \"{agent_scratchpad}\")\n",
        "        ])\n",
        "\n",
        "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
        "        return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    def create_analysis_agent(self):\n",
        "        \"\"\"Agent that analyzes patterns and trends from literature results\"\"\"\n",
        "        def analyze_papers_tool(query: str) -> str:\n",
        "            papers = self.shared_memory.get('papers', [])\n",
        "            if not papers:\n",
        "                return \"No papers available for analysis. Run literature search first.\"\n",
        "\n",
        "            # Perform analysis\n",
        "            analysis = f\"Analysis of {len(papers)} papers:\\n\\n\"\n",
        "\n",
        "            # Author analysis\n",
        "            all_authors = []\n",
        "            for paper in papers:\n",
        "                all_authors.extend(paper['authors'])\n",
        "            author_counts = pd.Series(all_authors).value_counts().head(10)\n",
        "            analysis += f\"Top Authors:\\n{author_counts.to_string()}\\n\\n\"\n",
        "\n",
        "            # Year analysis\n",
        "            years = []\n",
        "            for paper in papers:\n",
        "                try:\n",
        "                    year = int(paper['published'][:4])\n",
        "                    years.append(year)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if years:\n",
        "                year_counts = pd.Series(years).value_counts().sort_index()\n",
        "                analysis += f\"Publication Trends:\\n{year_counts.to_string()}\\n\\n\"\n",
        "\n",
        "            # Keyword extraction from titles and abstracts\n",
        "            text = \" \".join([p['title'] + \" \" + p['summary'] for p in papers])\n",
        "            words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
        "            word_counts = pd.Series(words).value_counts().head(20)\n",
        "            analysis += f\"Common Keywords:\\n{word_counts.to_string()}\\n\\n\"\n",
        "\n",
        "            # Store analysis for other agents\n",
        "            self.shared_memory['analysis'] = {\n",
        "                'author_counts': author_counts.to_dict(),\n",
        "                'year_counts': year_counts.to_dict() if years else {},\n",
        "                'keywords': word_counts.to_dict()\n",
        "            }\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        def identify_gaps_tool(query: str) -> str:\n",
        "            papers = self.shared_memory.get('papers', [])\n",
        "            if not papers:\n",
        "                return \"No papers available for gap analysis.\"\n",
        "\n",
        "            # Simple gap analysis based on keywords and trends\n",
        "            gaps = \"Research Gap Analysis:\\n\\n\"\n",
        "            gaps += \"Based on the literature, potential research gaps include:\\n\"\n",
        "            gaps += \"1. Underexplored methodologies\\n\"\n",
        "            gaps += \"2. Recent technological advances not yet applied\\n\"\n",
        "            gaps += \"3. Cross-disciplinary opportunities\\n\"\n",
        "            gaps += \"4. Geographical or demographic gaps in studies\\n\"\n",
        "\n",
        "            return gaps\n",
        "\n",
        "        tools = [\n",
        "            Tool(name=\"analyze_papers\", func=analyze_papers_tool,\n",
        "                 description=\"Analyze patterns in research papers\"),\n",
        "            Tool(name=\"identify_gaps\", func=identify_gaps_tool,\n",
        "                 description=\"Identify research gaps from literature\")\n",
        "        ]\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research analysis agent. Your role is to:\n",
        "            1. Analyze patterns in research literature\n",
        "            2. Identify trends and gaps\n",
        "            3. Provide insights for future research directions\n",
        "            Be analytical and provide concrete insights.\"\"\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"assistant\", \"{agent_scratchpad}\")\n",
        "        ])\n",
        "\n",
        "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
        "        return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    def create_writing_agent(self):\n",
        "        \"\"\"Agent that creates structured reports from research and analysis\"\"\"\n",
        "        def generate_report_tool(query: str) -> str:\n",
        "            papers = self.shared_memory.get('papers', [])\n",
        "            analysis = self.shared_memory.get('analysis', {})\n",
        "\n",
        "            if not papers:\n",
        "                return \"No research data available for report generation.\"\n",
        "\n",
        "            report = f\"\"\"# Literature Review Report: {self.shared_memory.get('search_query', 'Research Topic')}\n",
        "\n",
        "## Executive Summary\n",
        "This report analyzes {len(papers)} academic papers from {self.selected_database} database.\n",
        "\n",
        "## Key Findings\n",
        "- Total papers analyzed: {len(papers)}\n",
        "- Publication span: {min(analysis.get('year_counts', {}).keys()) if analysis.get('year_counts') else 'N/A'} - {max(analysis.get('year_counts', {}).keys()) if analysis.get('year_counts') else 'N/A'}\n",
        "- Most active authors: {list(analysis.get('author_counts', {}).keys())[:3] if analysis.get('author_counts') else 'N/A'}\n",
        "\n",
        "## Literature Overview\n",
        "\"\"\"\n",
        "\n",
        "            # Add paper summaries\n",
        "            for i, paper in enumerate(papers[:5], 1):  # Top 5 papers\n",
        "                report += f\"\\n### Paper {i}: {paper['title']}\\n\"\n",
        "                report += f\"**Authors:** {', '.join(paper['authors'][:3])}\\n\"\n",
        "                report += f\"**Published:** {paper['published']}\\n\"\n",
        "                report += f\"**Summary:** {paper['summary'][:300]}...\\n\"\n",
        "\n",
        "            report += f\"\\n## Trends and Patterns\\n\"\n",
        "            if analysis.get('keywords'):\n",
        "                top_keywords = list(analysis['keywords'].keys())[:10]\n",
        "                report += f\"**Key themes:** {', '.join(top_keywords)}\\n\"\n",
        "\n",
        "            report += f\"\\n## Recommendations\\n\"\n",
        "            report += \"Based on this literature review, we recommend:\\n\"\n",
        "            report += \"1. Further investigation into emerging themes\\n\"\n",
        "            report += \"2. Cross-disciplinary collaboration opportunities\\n\"\n",
        "            report += \"3. Addressing identified research gaps\\n\"\n",
        "\n",
        "            # Store report for download\n",
        "            self.shared_memory['final_report'] = report\n",
        "\n",
        "            return report\n",
        "\n",
        "        tools = [Tool(\n",
        "            name=\"generate_report\",\n",
        "            func=generate_report_tool,\n",
        "            description=\"Generate a structured research report\"\n",
        "        )]\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a scientific writing agent. Your role is to:\n",
        "            1. Create well-structured research reports\n",
        "            2. Synthesize findings from multiple sources\n",
        "            3. Provide clear recommendations\n",
        "            Write in academic style but keep it accessible.\"\"\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"assistant\", \"{agent_scratchpad}\")\n",
        "        ])\n",
        "\n",
        "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
        "        return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    def create_fact_check_agent(self):\n",
        "        \"\"\"Agent that fact-checks claims using external sources\"\"\"\n",
        "        # Initialize external search tools\n",
        "        wiki_search = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "        web_search = DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper())\n",
        "\n",
        "        def fact_check_claims_tool(query: str) -> str:\n",
        "            papers = self.shared_memory.get('papers', [])\n",
        "            if not papers:\n",
        "                return \"No research data available for fact-checking.\"\n",
        "\n",
        "            # Extract key claims from papers (simplified)\n",
        "            claims = []\n",
        "            for paper in papers[:3]:  # Check first 3 papers\n",
        "                # Simple claim extraction (in practice, you'd use more sophisticated NLP)\n",
        "                sentences = paper['summary'].split('. ')\n",
        "                for sentence in sentences[:2]:  # First 2 sentences\n",
        "                    if len(sentence) > 50:  # Substantial claims\n",
        "                        claims.append(sentence)\n",
        "\n",
        "            fact_check_results = \"Fact-Check Results:\\n\\n\"\n",
        "            for i, claim in enumerate(claims[:3], 1):  # Check first 3 claims\n",
        "                fact_check_results += f\"Claim {i}: {claim[:100]}...\\n\"\n",
        "\n",
        "                # Search for supporting evidence (simplified)\n",
        "                try:\n",
        "                    search_query = claim.split()[:5]  # First 5 words\n",
        "                    wiki_result = wiki_search.run(\" \".join(search_query))\n",
        "                    fact_check_results += f\"Supporting evidence found: {wiki_result[:200]}...\\n\\n\"\n",
        "                except:\n",
        "                    fact_check_results += \"Could not verify this claim.\\n\\n\"\n",
        "\n",
        "            return fact_check_results\n",
        "\n",
        "        tools = [\n",
        "            Tool(name=\"fact_check\", func=fact_check_claims_tool,\n",
        "                 description=\"Fact-check research claims using external sources\"),\n",
        "            wiki_search,\n",
        "            web_search\n",
        "        ]\n",
        "\n",
        "        agent = initialize_agent(\n",
        "            tools, self.llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        "        )\n",
        "        return agent\n",
        "\n",
        "    def create_visualization_agent(self):\n",
        "        \"\"\"Agent that creates visualizations from research data\"\"\"\n",
        "        def create_visualizations_tool(query: str) -> str:\n",
        "            analysis = self.shared_memory.get('analysis', {})\n",
        "            papers = self.shared_memory.get('papers', [])\n",
        "\n",
        "            if not analysis or not papers:\n",
        "                return \"No data available for visualization.\"\n",
        "\n",
        "            # Create publication timeline\n",
        "            if analysis.get('year_counts'):\n",
        "                years = list(analysis['year_counts'].keys())\n",
        "                counts = list(analysis['year_counts'].values())\n",
        "\n",
        "                fig = px.line(x=years, y=counts, title=\"Publication Timeline\")\n",
        "                self.shared_memory['timeline_chart'] = fig\n",
        "\n",
        "            # Create keyword cloud data\n",
        "            if analysis.get('keywords'):\n",
        "                self.shared_memory['keywords_data'] = analysis['keywords']\n",
        "\n",
        "            return \"Visualizations created and stored. Check the visualization section below.\"\n",
        "\n",
        "        tools = [Tool(\n",
        "            name=\"create_viz\",\n",
        "            func=create_visualizations_tool,\n",
        "            description=\"Create visualizations from research data\"\n",
        "        )]\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a data visualization agent. Create meaningful charts and graphs from research data.\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"assistant\", \"{agent_scratchpad}\")\n",
        "        ])\n",
        "\n",
        "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
        "        return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "RzsF-F8Z4Ehq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Streamlit Interface\n",
        "st.title(\"ðŸ¤– Multi-Agent Research System\")\n",
        "st.write(\"An AI system that uses multiple specialized agents to conduct comprehensive research analysis.\")\n",
        "\n",
        "# Display selected agents\n",
        "if agent_pipeline:\n",
        "    st.info(f\"ðŸŽ¯ **Active Agents:** {' â†’ '.join(agent_pipeline)}\")\n",
        "\n",
        "# Input section\n",
        "col1, col2 = st.columns([3, 1])\n",
        "with col1:\n",
        "    research_query = st.text_input(\n",
        "        \"Enter your research question:\",\n",
        "        placeholder=\"e.g., 'machine learning in healthcare'\"\n",
        "    )\n",
        "\n",
        "with col2:\n",
        "    execution_mode = st.selectbox(\n",
        "        \"Execution Mode\",\n",
        "        [\"Sequential\", \"Parallel Analysis\"]\n",
        "    )\n",
        "\n",
        "if st.button(\"ðŸš€ Start Multi-Agent Research\", type=\"primary\"):\n",
        "    if not openai_api_key:\n",
        "        st.error(\"Please enter your OpenAI API key.\")\n",
        "    elif not research_query:\n",
        "        st.error(\"Please enter a research query.\")\n",
        "    elif not agent_pipeline:\n",
        "        st.error(\"Please select at least one agent.\")\n",
        "    else:\n",
        "        # Initialize orchestrator\n",
        "        orchestrator = MultiAgentOrchestrator(openai_api_key, selected_db)\n",
        "\n",
        "        # Create agents based on selection\n",
        "        agents = {}\n",
        "        if \"Literature Agent\" in agent_pipeline:\n",
        "            agents[\"literature\"] = orchestrator.create_literature_agent()\n",
        "        if \"Analysis Agent\" in agent_pipeline:\n",
        "            agents[\"analysis\"] = orchestrator.create_analysis_agent()\n",
        "        if \"Writing Agent\" in agent_pipeline:\n",
        "            agents[\"writing\"] = orchestrator.create_writing_agent()\n",
        "        if \"Fact-Check Agent\" in agent_pipeline:\n",
        "            agents[\"fact_check\"] = orchestrator.create_fact_check_agent()\n",
        "        if \"Visualization Agent\" in agent_pipeline:\n",
        "            agents[\"visualization\"] = orchestrator.create_visualization_agent()\n",
        "\n",
        "        # Execute agents sequentially\n",
        "        results = {}\n",
        "\n",
        "        with st.spinner(\"ðŸ” Running multi-agent research pipeline...\"):\n",
        "            try:\n",
        "                # 1. Literature Agent (always first if selected)\n",
        "                if \"literature\" in agents:\n",
        "                    st.write(\"### ðŸ“š Literature Agent Working...\")\n",
        "                    lit_result = agents[\"literature\"].invoke({\n",
        "                        \"input\": f\"Search for academic papers about: {research_query}\"\n",
        "                    })\n",
        "                    results[\"literature\"] = lit_result[\"output\"]\n",
        "                    st.success(\"âœ… Literature search complete!\")\n",
        "\n",
        "                # 2. Analysis Agent\n",
        "                if \"analysis\" in agents:\n",
        "                    st.write(\"### ðŸ“Š Analysis Agent Working...\")\n",
        "                    analysis_result = agents[\"analysis\"].invoke({\n",
        "                        \"input\": f\"Analyze the research papers about: {research_query}\"\n",
        "                    })\n",
        "                    results[\"analysis\"] = analysis_result[\"output\"]\n",
        "                    st.success(\"âœ… Analysis complete!\")\n",
        "\n",
        "                # 3. Writing Agent\n",
        "                if \"writing\" in agents:\n",
        "                    st.write(\"### âœï¸ Writing Agent Working...\")\n",
        "                    writing_result = agents[\"writing\"].invoke({\n",
        "                        \"input\": f\"Generate a comprehensive report about: {research_query}\"\n",
        "                    })\n",
        "                    results[\"writing\"] = writing_result[\"output\"]\n",
        "                    st.success(\"âœ… Report generation complete!\")\n",
        "\n",
        "                # 4. Fact-Check Agent\n",
        "                if \"fact_check\" in agents:\n",
        "                    st.write(\"### ðŸ” Fact-Check Agent Working...\")\n",
        "                    fact_check_result = agents[\"fact_check\"].run(\n",
        "                        f\"Fact-check key claims from the research on: {research_query}\"\n",
        "                    )\n",
        "                    results[\"fact_check\"] = fact_check_result\n",
        "                    st.success(\"âœ… Fact-checking complete!\")\n",
        "\n",
        "                # 5. Visualization Agent\n",
        "                if \"visualization\" in agents:\n",
        "                    st.write(\"### ðŸ“ˆ Visualization Agent Working...\")\n",
        "                    viz_result = agents[\"visualization\"].invoke({\n",
        "                        \"input\": f\"Create visualizations for research on: {research_query}\"\n",
        "                    })\n",
        "                    results[\"visualization\"] = viz_result[\"output\"]\n",
        "                    st.success(\"âœ… Visualizations complete!\")\n",
        "\n",
        "                # Display all results\n",
        "                st.markdown(\"## ðŸŽ¯ Multi-Agent Research Results\")\n",
        "\n",
        "                for agent_name, result in results.items():\n",
        "                    with st.expander(f\"ðŸ“‹ {agent_name.title()} Results\", expanded=True):\n",
        "                        st.markdown(result)\n",
        "\n",
        "                # Show visualizations if created\n",
        "                if orchestrator.shared_memory.get('timeline_chart'):\n",
        "                    st.plotly_chart(orchestrator.shared_memory['timeline_chart'])\n",
        "\n",
        "                # Provide download for final report\n",
        "                if orchestrator.shared_memory.get('final_report'):\n",
        "                    st.download_button(\n",
        "                        label=\"ðŸ“„ Download Complete Research Report\",\n",
        "                        data=orchestrator.shared_memory['final_report'],\n",
        "                        file_name=f\"multi_agent_research_{research_query.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.md\",\n",
        "                        mime=\"text/markdown\"\n",
        "                    )\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred in the multi-agent pipeline: {e}\")\n",
        "\n",
        "# Agent Information\n",
        "with st.expander(\"â„¹ï¸ About the Multi-Agent System\"):\n",
        "    st.markdown(\"\"\"\n",
        "    ### ðŸ¤– Available Agents:\n",
        "\n",
        "    - **Literature Agent**: Searches academic databases for relevant papers\n",
        "    - **Analysis Agent**: Analyzes patterns, trends, and identifies research gaps\n",
        "    - **Writing Agent**: Creates structured reports and summaries\n",
        "    - **Fact-Check Agent**: Verifies claims using external sources (Wikipedia, web search)\n",
        "    - **Visualization Agent**: Creates charts and graphs from research data\n",
        "\n",
        "    ### ðŸ”„ How It Works:\n",
        "\n",
        "    1. **Sequential Processing**: Agents work in order, each building on previous results\n",
        "    2. **Shared Memory**: Agents share data through a common memory system\n",
        "    3. **Specialized Tools**: Each agent has specific tools for their domain\n",
        "    4. **Comprehensive Output**: Final report combines insights from all agents\n",
        "    \"\"\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"*Multi-Agent Research System - Built with LangChain, OpenAI, and Streamlit*\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4sz9vlm0HhNw",
        "outputId": "921558f0-c5e8-4385-fe22-cf120872fdb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-17 13:42:27.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.150 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.182 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.185 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-17 13:42:27.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}